<meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=no">
<title>Mustache</title>
<style>
  html, body {
    margin: 0 auto;
    height: 100%;
    background: rgb(51, 51, 58);
    color: #fff;
  }
  video {
    display: none;
  }
  canvas {
    width: 100vw;
    height: 100vh;
  }
  pre {
    position: absolute;
    top: 24px;
    left: 24px;
    white-space: pre-line;
    right: 24px;
    text-shadow: 0 0 black;
   }
  .recording {
    background-color: #F44336;
  }
  #recordButton {
    position: absolute;
    bottom: 48px;
    right: 0;
    text-align: center;
    left: 0;
    width: 64px;
    margin: auto;
    font-size: 48px;
    background: rgba(0, 193, 233, 0.5);
    line-height: 64px;
    border-radius: 50%;
    box-shadow: 0 0 4px 2px rgb(51, 51, 58);
  }
</style>
<canvas id="canvas"></canvas>
<video id="video" autoplay muted></video>
<div id="recordButton">&#9210;</div>
<pre id="log"></pre>
<script>

let stream;
let faces = [];
let recorder;
let chunks = [];

(async _ => {

  // Grab camera stream.
  const constraints = {
     video: { facingMode: 'user' }
  };
  stream = await navigator.mediaDevices.getUserMedia(constraints);
  video.srcObject = stream;
  await video.play();

  canvas.width = window.innerWidth; // * devicePixelRatio;
  canvas.height = window.innerHeight; // * devicePixelRatio;
  requestAnimationFrame(draw);

  recordButton.addEventListener('click', onRecordButtonClick);

})();

async function draw() {
  const context = canvas.getContext('2d');
  context.clearRect(0, 0, canvas.width, canvas.height);

  // Draw video frame.
  const ratio  = Math.min(canvas.width / video.videoWidth, canvas.height / video.videoHeight);
  context.drawImage(video, 0, 0, video.videoWidth, video.videoHeight,
      (canvas.width - video.videoWidth * ratio) / 2, 0,
      video.videoWidth * ratio, video.videoHeight * ratio);  

  const faceDetector = new FaceDetector({ fastMode: true, maxDetectedFaces: 1 });
  const imageData = context.getImageData(0, 0, canvas.width, canvas.height);

  // Draw previously detected faces.
  for (const face of faces) {
    context.beginPath();
    context.rect(face.boundingBox.x, face.boundingBox.y, face.boundingBox.width, face.boundingBox.height);
    context.lineWidth = 1 * devicePixelRatio;
    context.strokeStyle = 'deeppink';
    //context.closePath();
    context.stroke();
  }

  // Detect new faces.
  faces = await faceDetector.detect(imageData);

  requestAnimationFrame(draw);
}

function onRecordButtonClick(event) {
  if (document.body.classList.toggle('recording')) {
    recorder = new MediaRecorder(stream, { mimeType: 'video/webm;codecs=h264' });
    recorder.start(10); // collect 10ms of data
    recorder.addEventListener('dataavailable', event => { chunks.push(event.data); });
    recorder.addEventListener('stop', event => {
      const blob = new Blob(chunks, { type: 'video/webm' });
      window.open(URL.createObjectURL(blob));
    });
  } else {
    recorder.stop();
  }
}

window.addEventListener('unhandledrejection', event => {
  log.textContent = event.reason;
});
</script>

